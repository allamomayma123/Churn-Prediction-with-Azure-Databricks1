{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9144f198-b4c6-4768-a975-004e87d530f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Initialize Spark session and load data\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"HR Employee Churn\").getOrCreate()\n",
    "\n",
    "# Define file location and type\n",
    "file_location = \"/FileStore/tables/hr_employee_churn_data.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# Load the data into a Spark DataFrame\n",
    "df_spark = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"sep\", \",\") \\\n",
    "  .load(file_location)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df_spark.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b438e01f-f7b6-46f5-9bda-34fa46731f4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Initial data exploration with Spark SQL\n",
    "\n",
    "# Create a temporary view\n",
    "df_spark.createOrReplaceTempView(\"hr_employee_churn\")\n",
    "\n",
    "# Use Spark SQL to query the data\n",
    "query = \"\"\"\n",
    "SELECT COUNT(*) AS total_records,\n",
    "       AVG(satisfaction_level) AS avg_satisfaction_level,\n",
    "       AVG(last_evaluation) AS avg_last_evaluation,\n",
    "       AVG(number_project) AS avg_number_project,\n",
    "       AVG(average_montly_hours) AS avg_monthly_hours,\n",
    "       AVG(time_spend_company) AS avg_time_spent,\n",
    "       SUM(CASE WHEN left = 1 THEN 1 ELSE 0 END) AS total_churn\n",
    "FROM hr_employee_churn\n",
    "\"\"\"\n",
    "\n",
    "# Run the query and show the results\n",
    "df_summary = spark.sql(query)\n",
    "df_summary.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c177543e-4b2e-4d36-8d2c-af0564af9130",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Data preprocessing with PySpark\n",
    "\n",
    "from pyspark.sql.functions import mean, col\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "# Handle missing values\n",
    "mean_satisfaction_level = df_spark.select(mean(col('satisfaction_level'))).collect()[0][0]\n",
    "df_spark = df_spark.na.fill({'satisfaction_level': mean_satisfaction_level})\n",
    "\n",
    "# Encode categorical variables using StringIndexer and OneHotEncoder\n",
    "indexer = StringIndexer(inputCol=\"salary\", outputCol=\"salaryIndex\")\n",
    "df_spark = indexer.fit(df_spark).transform(df_spark)\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[\"salaryIndex\"], outputCols=[\"salaryVec\"])\n",
    "df_spark = encoder.fit(df_spark).transform(df_spark)\n",
    "\n",
    "# Use VectorAssembler to combine all feature columns into a single vector column\n",
    "assembler = VectorAssembler(inputCols=['satisfaction_level', 'last_evaluation', 'number_project', \n",
    "                                       'average_montly_hours', 'time_spend_company', 'Work_accident', \n",
    "                                       'promotion_last_5years', 'salaryVec'], outputCol=\"features\")\n",
    "df_spark = assembler.transform(df_spark)\n",
    "\n",
    "# Drop the intermediate columns\n",
    "df_spark = df_spark.drop(\"salary\", \"salaryIndex\", \"salaryVec\")\n",
    "\n",
    "# Show the processed Spark DataFrame\n",
    "df_spark.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcfbd1f8-4832-4a77-9742-9868c599acf2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Convert Spark DataFrame to Pandas for further processing\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "df_pandas = df_spark.select(\"empid\", \"features\", \"left\").toPandas()\n",
    "\n",
    "# Function to extract features from the vector column\n",
    "def extract_features(features_vector):\n",
    "    return features_vector.toArray()\n",
    "\n",
    "# Apply the function to extract features\n",
    "features_array = np.array(df_pandas['features'].apply(lambda x: x.toArray()).tolist())\n",
    "\n",
    "# Define feature names\n",
    "feature_names = ['satisfaction_level', 'last_evaluation', 'number_project', \n",
    "                 'average_montly_hours', 'time_spend_company', 'Work_accident', \n",
    "                 'promotion_last_5years', 'salaryVec_0', 'salaryVec_1']\n",
    "\n",
    "# Create a DataFrame from the features array\n",
    "features_df = pd.DataFrame(features_array, columns=feature_names)\n",
    "\n",
    "# Combine the features DataFrame with the target and ID columns\n",
    "df_pandas = pd.concat([df_pandas[['empid', 'left']], features_df], axis=1)\n",
    "\n",
    "# Show the first few rows of the Pandas DataFrame\n",
    "print(df_pandas.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc331a0c-f3f1-4515-b017-321a5f372d8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pandas.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67c583a5-5011-4f72-90dd-ac56cb633f8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the distribution of the target variable 'left'\n",
    "sns.countplot(x='left', data=df_pandas)\n",
    "plt.title('Distribution of Employee Churn')\n",
    "plt.show()\n",
    "\n",
    "# Plot the correlation matrix\n",
    "corr_matrix = df_pandas.corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Plot distributions of numerical features\n",
    "numerical_features = ['satisfaction_level', 'last_evaluation', 'number_project', 'average_montly_hours', 'time_spend_company']\n",
    "for feature in numerical_features:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(df_pandas[feature], kde=True)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d92cbd8-5ce6-4c28-8cc2-25b3609c58fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 6: Machine Learning Model Building with Scikit-Learn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Define features and target variable\n",
    "X = df_pandas.drop(columns=['empid', 'left'])\n",
    "y = df_pandas['left']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ca06ea6-d53a-4714-9d49-71728ca43f54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "058b92b5-66d0-4a9c-8c8f-399b8ee45730",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e28828b-4a83-4bc9-a765-f78c91d4827e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bb1b729-a687-436e-a963-7fe81ebacf74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train and evaluate models\n",
    "results = []\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = model.score(X_test_scaled, y_test)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    results.append((model_name, accuracy, roc_auc))\n",
    "    print(f'{model_name} - Accuracy: {accuracy:.4f}, ROC-AUC: {roc_auc:.4f}')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix for {model_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results, columns=['Model', 'Accuracy', 'ROC-AUC'])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61240bd1-2204-4d2c-97a9-9a18d4193b8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import mlflow\n",
    "\n",
    "# Define parameter grid for RandomForestClassifier\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=models['Random Forest'], param_grid=param_grid, \n",
    "                           cv=5, scoring='roc_auc', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Perform grid search with MLflow logging\n",
    "with mlflow.start_run(run_name=\"Random Forest - Grid Search\"):\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    best_rf_model = grid_search.best_estimator_\n",
    "    y_pred = best_rf_model.predict(X_test_scaled)\n",
    "    accuracy = best_rf_model.score(X_test_scaled, y_test)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    # Log parameters, metrics, and best model\n",
    "    mlflow.log_param(\"model_name\", \"Random Forest - Grid Search\")\n",
    "    mlflow.log_params(grid_search.best_params_)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "    mlflow.sklearn.log_model(best_rf_model, \"Random Forest - Grid Search\")\n",
    "    \n",
    "    print(f'Random Forest - Grid Search - Accuracy: {accuracy:.4f}, ROC-AUC: {roc_auc:.4f}')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix for Random Forest - Grid Search')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ad69c92-6c9d-4818-8e3b-adaf1b331751",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Churn_Model_Dev",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
